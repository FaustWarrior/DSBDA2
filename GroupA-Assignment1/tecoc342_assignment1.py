# -*- coding: utf-8 -*-
"""TECOC342_Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uNG0pq9sd60ECjiUhBFKrYh78BwW67-P

NAME: **Sumedha Nitin Zaware**

ROLL NO.: **TECOC342**



***ASSIGNMENT-1***

Perform the following operations using Python on any open source dataset (e.g., data.csv)

1. Import all the required Python Libraries.
2. Locate an open source data from the web (e.g. https://www.kaggle.com). Provide a. clear description of the data and its source (i.e., URL of the web site).
3. Load the Dataset into pandas data frame.
4. Data Preprocessing: check for missing values in the data using pandas insull(), describe() function to get some initial statistics. Provide variable descriptions. Types of variables etc. Check the dimensions of the data frame.
5. Data Formatting and Data Normalization: Summarize the types of variables by checking the data types (i.e., character, numeric, integer, factor, and logical) of the variables in the data set. If variables are not in the correct data type, apply proper type conversions.
6. Turn categorical variables into quantitative variables in Python.
In addition to the codes and outputs, explain every operation that you do in the above steps and explain everything that you do to import/read/scrape the data set.

##Part-1

**Data Wrangling**

Data Wrangling is the process of converting data from the initial format to a format that may be better for analysis.

Import the required libraries
"""

import pandas as pd
import matplotlib.pylab as plt
import numpy as np

"""Load the dataset using pandas library"""

df = pd.read_csv("autodata.csv")

"""Check the contents of dataset using **df.head()** and **df.tail()** functions"""

df.head(5)

df.tail(5)

df.info()

df.describe()

"""**Evaluating for Missing Data**

The missing values are converted to Python's default. We use Python's built-in functions to identify these missing values. There are two methods to detect missing data:

- isnull()
- .notnull()

The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data. "True" stands for missing value, while "False" stands for not missing value.

**Deal with missing data**

1. **Drop data**
  - Drop the whole row
  - Drop the whole column

2. **Replace data**
  - Replace it by mean
  - Replace it by frequency / mode
  - Replace it based on other functions
"""

df.isnull()

df.isnull().sum()

df.notnull()

df.notnull().sum()

"""Based on the summary above, each column has 205 rows of data, seven columns containing missing data:

**stroke** : 4 missing data

**horsepower**: 2 missing data

**peak-rpm**: 2 missing data

**horsepower-binned**: 2 missing data

**Question 1**
"""

# calculate the mean vaule for "stroke" column
avg_stroke = df["stroke"].astype("float").mean(axis = 0)
print("Average of stroke:", avg_stroke)

# replace NaN by mean value in "stroke" column
df["stroke"].replace(np.nan, avg_stroke, inplace = True)

"""Calculate the mean value for the 'horsepower' column:"""

avg_hp = df["horsepower"].astype("float").mean(axis = 0)
print("Average of stroke:", avg_hp)

"""Replace "NaN" by mean value:"""

df["horsepower"].replace(np.nan, avg_hp, inplace = True)

"""Calculate the mean value for 'peak-rpm' column:"""

avg_rpm = df["peak-rpm"].astype("float").mean(axis = 0)
print("Average of stroke:", avg_rpm)

"""Replace NaN by mean value:"""

df["peak-rpm"].replace(np.nan, avg_hp, inplace = True)

df['num-of-doors'].value_counts()

df['num-of-doors'].value_counts().idxmax()

#replace the missing 'num-of-doors' values by the most frequent 
df["num-of-doors"].replace(np.nan, "four", inplace=True)

# simply drop whole row with NaN in "horsepower-binned" column
df.dropna(subset=["horsepower-binned"], axis=0, inplace=True)

# reset index, because we droped two rows
df.reset_index(drop=True, inplace=True)

df.isnull().sum()

"""Here, we can see that there are no missing values in the data.

##Part-2

##Data Standardization
Data is usually collected from different agencies with different formats. (Data Standardization is also a term for a particular type of data normalization, where we subtract the mean and divide by the standard deviation)

**What is Standardization?**

Standardization is the process of transforming data into a common format which allows the researcher to make the meaningful comparison.

**Example:**

Transform mpg to L/100km:

In our dataset, the fuel consumption columns "city-mpg" and "highway-mpg" are represented by mpg (miles per gallon) unit. Assume we are developing an application in a country that accept the fuel consumption with L/100km standard
"""

df['city-L/100km'] = 235/df["city-mpg"]
df.head()

df['highway-L/100km'] = 235/df["highway-mpg"]
df.head()

"""##Data Normalization

Normalization is the process of transforming values of several variables into a similar range. Typical normalizations include scaling the variable so the variable average is 0, scaling the variable so the variance is 1, or scaling variable so the variable values range from 0 to 1

**Example:**

To demonstrate normalization, let's say we want to scale the columns "length", "width" and "height"

Target:would like to Normalize those variables so their value ranges from 0 to 1.

"""

df['length'] = df['length']/df['length'].max()
df['width'] = df['width']/df['width'].max()

df['height'] = df['height']/df['height'].max() 
df[["length","width","height"]].head()

"""**Indicator variable (or dummy variable)**

An indicator variable (or dummy variable) is a numerical variable used to label categories. They are called 'dummies' because the numbers themselves don't have inherent meaning.

**Why we use indicator variables?**

So we can use categorical variables for regression analysis in the later modules.

**Example:**

We see the column "fuel-type" has two unique values, "gas" or "diesel". Regression doesn't understand words, only numbers. To use this attribute in regression analysis, we convert "fuel-type" into indicator variables.

We will use the panda's method 'get_dummies' to assign numerical values to different categories of fuel type.
"""

df.columns

df['aspiration'].value_counts()

dummy_variable_1 = pd.get_dummies(df["aspiration"])
dummy_variable_1.head()

"""We now have the value 0 to represent "turbo" and 1 to represent "std" in the column "aspiration". We will now insert this column back into our original dataset."""

df = pd.concat([df, dummy_variable_1], axis=1)
df.drop("aspiration", axis = 1, inplace=True)

df.head()

"""The last two columns are now the indicator variable representation of the aspiration variable. It's all 0s and 1s now.

##Binning
Binning is a process of transforming continuous numerical variables into discrete categorical 'bins', for grouped analysis.

**Example:**

In our dataset, "horsepower" is a real valued variable ranging from 48 to 288, it has 57 unique values. What if we only care about the price difference between cars with high horsepower, medium horsepower, and little horsepower (3 types)? Can we rearrange them into three â€˜bins' to simplify analysis?

### Horsepower
"""

df["horsepower"]=df["horsepower"].astype(float, copy=True)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(df["horsepower"])

plt.pyplot.xlabel("horsepower")
plt.pyplot.ylabel("count")
plt.pyplot.title("horsepower bins")

bins = np.linspace(min(df["horsepower"]), max(df["horsepower"]), 4)
bins

group_names = ['Low', 'Medium', 'High']

df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True )
df[['horsepower','horsepower-binned']].head(20)

df["horsepower-binned"].value_counts()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
pyplot.bar(group_names, df["horsepower-binned"].value_counts())

# set x/y labels and plot title
plt.pyplot.xlabel("horsepower")
plt.pyplot.ylabel("count")
plt.pyplot.title("horsepower bins")

"""### Peak-rpm"""

df["peak-rpm"]=df["peak-rpm"].astype(float, copy=True)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(df["peak-rpm"])

plt.pyplot.xlabel("peak-rpm")
plt.pyplot.ylabel("count")
plt.pyplot.title("Peak-rpm bins")

bins = np.linspace(min(df["peak-rpm"]), max(df["peak-rpm"]), 4)
bins

group_names1 = ['Low', 'Medium', 'High']

df['peakrpm-binned'] = pd.cut(df['peak-rpm'], bins, labels=group_names, include_lowest=True )
df[['peak-rpm','peakrpm-binned']].head(20)

df["peakrpm-binned"].value_counts()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
pyplot.bar(group_names, df["peakrpm-binned"].value_counts())

# set x/y labels and plot title
plt.pyplot.xlabel("Peak-rpm")
plt.pyplot.ylabel("count")
plt.pyplot.title("peak-rpm bins")

"""### Wheel-base"""

df["wheel-base"]=df["wheel-base"].astype(float, copy=True)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(df["wheel-base"])

plt.pyplot.xlabel("wheel-base")
plt.pyplot.ylabel("count")
plt.pyplot.title("Wheel-base bins")

bins = np.linspace(min(df["wheel-base"]), max(df["wheel-base"]), 4)
bins

group_names = ['Low', 'Medium', 'High']

df['wheelbase-binned'] = pd.cut(df['wheel-base'], bins, labels=group_names, include_lowest=True )
df[['wheel-base','wheelbase-binned']].head(20)

df["wheelbase-binned"].value_counts()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as plt
from matplotlib import pyplot
pyplot.bar(group_names, df["wheelbase-binned"].value_counts())

# set x/y labels and plot title
plt.pyplot.xlabel("Wheelbase")
plt.pyplot.ylabel("count")
plt.pyplot.title("Wheelbase bins")